\setcounter{chapter}{2}
\setcounter{lecture}{16}
\chapter{Caches}
\lecture{17}{14 Mar. 12:00}{Introduction to Caches}
\section{Memory Hierarchy}
We often need a lot of memory, LC2K alone can handle \(2^{18} \) bytes of memory. We have several choices for memory:
\begin{itemize}
	\item \textbf{SRAM}: Static Random Access Memory
		\begin{itemize}
			\item fast: ~2ns access time or faster
			\item decoders are big
			\item expensive, high area requirement
		\end{itemize}
	\item \textbf{DRAM}: Dynamic Random Access Memory
		\begin{itemize}
			\item slower: ~50ns access time
			\item must stall for dozens of cycles each memory load
			\item less expensive
		\end{itemize}
	\item \textbf{Flash}
		\begin{itemize}
			\item slow: access time varies wildly
			\item less expensive than DRAM
			\item non-volatile
		\end{itemize}
	\item \textbf{Disks}
		\begin{itemize}
			\item obnoxiously slow: 3,000,000ns access time
			\item dirt cheap
			\item non-volatile
		\end{itemize}
\end{itemize}

As seen above, there are trade-offs among each type of memory. Ideally, we would have cheap \emph{and} fast memory. So, we can use a combination of memory types to optimize the common case via strategic \textbf{locality of reference}.

\newpage

\begin{figure}[ht]
	\centering
	\incfig[0.6]{memhier}
	\caption{Caches Overview}
	\label{fig:caches-overview}
\end{figure}

\begin{definition}
	The \textbf{architectural} view of memory is what the machine language (or programmer) sees, i.e. just a big array.
\end{definition}
\begin{note}
	Breaking up the memory system into different pieces (cache, main memory, disk) is \textbf{not architectural}. The machine language doesn't know about it.
\end{note}

Can use our variety of memories as follows:
\begin{itemize}
	\item use small array of SRAM for the \textbf{cache}
	\item use a larger amount of DRAM for \textbf{main memory}
	\item use a lot of flash and/or disk for \textbf{virtual memory}
\end{itemize}

\section{Cache Basics}

Whenever memory returns data, we can store it in a cache. We'll need to store:
\begin{itemize}
	\item the data
	\item a tag denoting its memory location
	\item a "valid" status bit
\end{itemize}

Then for our next memory access, we can first check if the tag matches the address we are attempting to access.

% \begin{figure}
% 	\centering
% 	\incfig[0.3]{simplecache}
% 	\caption{The simplest cache}
% 	\label{fig:simple-cache}
% \end{figure}

\begin{definition}[Cache Hit]
	A \textbf{hit} occurs when data for a memory access is found in the cache.
\end{definition}

\begin{definition}[Cache Miss]
	A \textbf{miss} occurs when data for a memory access is \emph{not} found in the cache.
\end{definition}

\begin{definition}[Hit/Miss Rate]
	The \textbf{hit/miss rate} is the percentage of memory accesses that hit/miss in the cache.
\end{definition}

\newpage

\subsection{CAMs}

\begin{definition}
	\textbf{CAMs: content addressable memories} are akin to a set of data matching a query. Instead of an address we send a key to the memory, asking whether the key exists and, if so, what value it is associated with. Memory answers: yes/no and gives associated value (if there is one).
\end{definition}

We apply \emph{operations} on CAMS:
\begin{itemize}
	\item \textbf{Search}: the primary way to access a CAM
		\begin{itemize}
			\item send data to CAM memory
			\item return "found" or "not found"
			\item if found, return location of where it was found or its associated value
		\end{itemize}
	\item \textbf{Write}: send data for CAM to remember
\end{itemize}

\subsection{Cache Organization}

Cache memory can copy data from any part of main memory. Cache memory has two parts:
\begin{itemize}
	\item the \textbf{tag (CAM)}: holds the memory address
	\item the \textbf{block (SRAM)}: holds the memory data
\end{itemize}

A \textbf{hit} in the cache occurs when a tag match is found. The microprocessor sends the address to the CAM containing the tags and searches for the tag. If there's a search result hit, the corresponding block is forwarded to the microprocessor. If not, the address is forwarded to main memory:

\begin{figure}[h]
	\centering
	\incfig[0.9]{hitmiss}
	\caption{Hardware view of caches.}
	\label{fig:hardware-cache}
\end{figure}

\begin{problem}
	Given:
	\begin{itemize}
		\item \emph{cache} has 1 cycle access time
		\item \emph{main memory} has 100 cycle access time
		\item \emph{disk} has 10,000 cycles access time
	\end{itemize}
	What is the average access time for 100 memory references if 90\% of the cache accesses are hits and 80\% of the accesses to main memory are hits? Assume main memory access time includes tag array access to determine hit/miss.
\end{problem}
\begin{answer}
	\(0.9 \cdot 1 + 0.1 \cdot (100 + 0.2 \cdot 10000) = 210.9\) 
\end{answer}

\subsection{Cache Operation}
Every cache \emph{miss} will get the data from memory and \emph{allocate} a cache line to put the data in (just like any CAM write). However... what do we replace in the cache? Does an optimal replacement policy exist?

\begin{definition}[Temporal locality]
	The principle of \textbf{temporal locality} in program references says that if you access a memory location (e.g., 1000) you will be more likely to re-access that location than you will be to reference some other random location.
\end{definition}

\begin{remark}
	Locality is a property of \emph{programs} (not hardware).
\end{remark}

Specifically, temporal locality says that the \textbf{least recently referenced (LRU)} cache line should be \emph{evicted} to make room for the new line. Because the re-access probability falls over time as a cache line isn't referenced, the LRU line is least likely to be re-referenced.

\begin{definition}[Average Access Latency]
	Average Access Latency = cache latency \(\cdot\) hit rate \(+\) memory latency \(\cdot\)
\end{definition}

