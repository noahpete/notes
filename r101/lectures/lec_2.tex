\lecture{19}{16 Mar. 9:00}{Vectors and Approximating Nonlinear Equations}
\section{Vectors and Nonlinear Equations}

We can use vectors for linear approximations by understanding partial derivatives. Given nonlinear funcitons \(f: \mathbb{R}^m \to \mathbb{R}^m \) we want the linear approximation at \(x_0 \in \mathbb{R}^m \).
\[
  f(x) \approx f(x_0) + A(x - x_0)
\]
where \(A_{n \cross m}, x, x_0 \in \mathbb{R}^m\), and \(f(x_0), f(x) \in \mathbb{R}^n \). Here, \(A\) represents a matrix made up of partial derivatives.

\begin{remark}
  Everything is the same as finding a linear approximation at a point. We are just replacing the slope with a matrix and the \(x\)s with vectors.
\end{remark}

\section{Partial Derivatives}

Set \(x = x_0 + he_i\) where \(he_i\) is some small outside adjustment to \(x_0\) such that all \(x_0\) remain the same except the \(i\)-th component.

\[
  x_0 + he_i =
    \begin{bmatrix}
      x_{01} \\
      x_{02} \\
      \vdots \\
      x_{0m} \\
    \end{bmatrix} + h
    \begin{bmatrix}
      0 \\
      0 \\
      \vdots \\
      1 \\
      \vdots \\
      0
    \end{bmatrix} =
    \begin{bmatrix}
      x_{01} \\
      x_{02} \\
      \vdots \\
      x_{0i} + h \\
      \vdots \\
      x_{0m} \\
    \end{bmatrix}
    \text{    where \(h\) is small}
\]

Equivalently, we have:
\[
  f(x_0 + he_i) \approx f(x_0) + A(x_0 + he_i - x_0) = f(x_0) + h a_i^{col}
\]
where we can now solve for \(a_i^{col}\), which represents the derivative of \(f\) with respect to \(x_i\).

We can represent the numerical approximation of a partial derivative similar to how we represented the numerical approximation of a standard derivative. A partial derivative is represented with the mathematical symbol del: \(\partial\).

\section{Jacobian}
Given the nonlinear functions \(f: \mathbb{R}^m \to \mathbb{R}^m\), the \textbf{Jacobian} is
\[
  \frac{\partial f(x)}{\partial x} \coloneqq \Bigg [ \frac{\partial f(x)}{\partial x_1} \frac{\partial f(x)}{\partial x_2} \ldots \frac{\partial f(x)}{\partial x_m} \Bigg ]_{n \cross m}.
\]
\begin{itemize}
  \item The partial derivatives are stacked to form a matrix
  \item For each \(x \in \mathbb{R}^m \), \(\frac{\partial f(x)}{\partial x}\) is an \(n \cross m\) matrix
  \item The \textbf{gradient} of \(f: \mathbb{R}^m \to \mathbb{R} \) is a special Jacobian that for each \(x \in \mathbb{R}^m \), \(\nabla f(x)\) is a \(1 \cross m\) matrix 
\end{itemize}

\(f: \mathbb{R}^m \to \mathbb{R}^n\) looks like:
\[
  f(x) =
  \begin{bmatrix}
    f_1(x) \\
    f_2(x) \\
    \vdots \\
    f_n(x) \\
  \end{bmatrix}
\]

\(\frac{\partial f(x)}{\partial x}\) looks like:
\[
  \frac{\partial f(x)}{\partial x} = \left[
    \begin{array}{cccc}
     \frac{\partial f_1(x)}{\partial x_1} & \frac{\partial f_1(x)}{\partial x_2} & \ldots & \frac{\partial f_1(x)}{\partial x_m} \\
     \vdots & \vdots & \ddots & \vdots \\
     \frac{\partial f_n(x)}{\partial x_1} & \frac{\partial f_n(x)}{\partial x_2} & \ldots & \frac{\partial f_n(x)}{\partial x_m} \\
    \end{array}
    \right]
\] 

The linear approximation of nonlinear functions \(f: \mathbb{R}^m \to \mathbb{R}^n\) at point \(x_0 \in \mathbb{R}^m\) is
\[
  f(x) \approx f(x_0) + A(x - x_0) = f(x_0) + \frac{\partial f(x_0)}{\partial x} (x - x_0).
\]

\begin{problem}
  We have two functions:
  \begin{align*}
    f_1(x_1, x_2) &= \log (x_1) + \sqrt{x_2} \\
    f_2(x_1, x_2) &= x_1 \cdot x_2
  \end{align*}
  and let \(x_0 =
  \begin{bmatrix}
    1 \\
    2 \\  
  \end{bmatrix}\). 
\end{problem}
\begin{answer}
  \(f: \mathbb{R}^2 \to \mathbb{R}^2\) so \(\frac{\partial f(x)}{\partial x} \) is a \(2 \cross 2\) matrix. Using forward approximation we have \(\frac{\partial f(x_0)}{\partial x} = \frac{f(x_0 + he_i) - f(x_0)}{h} \) with \(h = 0.001\).
  \begin{itemize}
    \item \(f_1\) w.r.t \(x_1\):
    \[
      \frac{\partial f_1(x_1,x_2)}{\partial x_1} =
      \frac{(\log (1 + 0.001) + \sqrt{2}) - (\log (1) + \sqrt{2})}{0.001} =
      0.43408
    \]
    \item \(f_1\) w.r.t. \(x_2\):
    \[
      \frac{\partial f_1(x_1,x_2)}{\partial x_2} =
      \frac{(\log (1) + \sqrt{2 + 0.0001}) - (\log (1) + \sqrt{2})}{0.001} =
      0.35351
    \]
    \item \(f_2\) w.r.t. \(x_1\):
    \[
      \frac{\partial f_1(x_1,x_2)}{\partial x_1} =
      \frac{(1.001)(2) - (1)(2)}{0.001} = 2
    \]
    \item \(f_2\) w.r.t. \(x_2\):
    \[
      \frac{\partial f_1(x_1,x_2)}{\partial x_2} =
      \frac{(1)(2.001) - (1)(2)}{0.001} = 1
    \]
  \end{itemize}
  So, the linear approximation is \(f(x) \approx f(x_0) + \frac{\partial f(x_0)}{\partial x} (x - x_0)\)
  \begin{align*}
    f(x) &\approx f(x_0) + \frac{\partial f(x_0)}{\partial x}(x-x_0) \\
    &=
    \begin{bmatrix}
      \log (1) + \sqrt{2} \\
      (1)(2) \\
    \end{bmatrix} + \left[
    \begin{array}{cc}
      0.43408 & 0.35351 \\
      2 & 1 \\
    \end{array} \right]
    \begin{bmatrix}
      x_1 - 1 \\
      x_2 - 2 \\
    \end{bmatrix}
  \end{align*}
\end{answer}

\section{Newton-Raphson Method}

Just as Newton's Method was a useful tool for linear approximations, we can use the \textbf{Newton-Raphson Method} for \emph{non-linear approximations}. Given \(f: \mathbb{R}^n \to \mathbb{R}^n\), we want to find a root \(f(x_0) = 0\). Using our approximation above, we can substitute in \(x_{k+1}\) and solve for it:
\[
  x_{k+1} = x_k - \frac{\partial f(x)}{\partial x}^{-1} f(x_k)
\]

\begin{remark}
  Similarly to when we were solving \(Ax - b = 0\) we made sure \(\det (A) \neq 0\), we want to make sure \(\det (\frac{\partial f(x_k)}{\partial x} \neq 0)\).
\end{remark}

We can find \(\Delta x_k\) to avoid the inverses in the equations above:
\[
  \Delta x_k = - \left( \frac{\partial f(x_k)}{\partial x}  \right)^{-1} f(x_k)
\]

  Instead of the inverses or dividing matrices, we solve for \(\Delta x_k\) using LU or QR factorization. This can be done using the Newton-Raphson algorithm:

\begin{algorithm}
  \DontPrintSemicolon
  \caption{Newton-Raphson Algorithm}
  \KwData{\(f\)}
  \(F \gets \mathbf{LU} \left( \frac{\partial f(x_k)}{\partial x}  \right) \) \Comment*[r]{find \(\Delta x_k\) }
  \(y \gets \mathbf{ForwardSub} \left( F.L, F.P \cdot -f(x_k) \right) \) \\
  \(\Delta x_k \gets \mathbf{BackwardSub} \left( F.U, y \right) \) \\
  \(x_{k + 1} \gets x_k + \Delta x_k \) \Comment*[r]{use \(\Delta x_k\) to find \(x_{k + 1}\) }
  \If{\(f(x_{k+1}) = 0\)}{
    \Return{\(x_{k+1}\)\text{ as the root}}
  }
  \Else{
    loop back to \textbf{1}.
  }
\end{algorithm}

If we replace \(x_{k+1} = x_k \Delta x_k \) with
\[
  x_{k+1} = x_k + \varepsilon \Delta x_k
\]
we get the \textbf{Damped Newton-Raphson Method} for \(\varepsilon > 0\) (usually \(\varepsilon = 0.1\) is sufficient). This prevents \(\Delta x_k\) from being too big by decreasing the step size.